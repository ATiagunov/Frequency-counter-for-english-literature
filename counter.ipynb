{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_dir = input(\"Enter the path to your text file: \\n\")\n",
    "# while(os.path.isfile(txt_dir) == 0):\n",
    "#     print(\"File does not exist\")\n",
    "#     txt_dir = input(\"Enter the path to your text file or Ctrl-D to exit \\n\")\n",
    "txt_dir = \"fellowship.txt\"\n",
    "\n",
    "num_freq = int(\n",
    "    input(\"Choose list of the most frequent words: 3000, 5000, 7000, 10000 \\n\"))\n",
    "while(num_freq != 3000 and num_freq != 5000 and num_freq != 7000 and num_freq != 10000):\n",
    "    print(\"Invalid number\")\n",
    "    txt_dir = input(\n",
    "        \"Choose list of the most frequent words: 3000, 5000, 7000, 10000 or enter Ctrl-D to exit \\n\")\n",
    "freq_path = str(num_freq) + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNER(sentences):\n",
    "    tagger = SequenceTagger.load(\"/home/alexander/.flair/models/ner-english/ner\")\n",
    "    named_entities = []\n",
    "    for sent in sentences:\n",
    "        sentence = Sentence(sent)\n",
    "        tagger.predict(sentence)\n",
    "        for entity in sentence.get_spans('ner'):\n",
    "            named_entities.append(entity.text)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-25 15:32:20,239 loading file /home/alexander/.flair/models/ner-english/ner\n"
     ]
    }
   ],
   "source": [
    "with open(txt_dir, \"r\") as txt, open(freq_path, \"r\") as frq:\n",
    "    text = txt.read()\n",
    "    freqs = frq.read().split()\n",
    "    sentences = sent_tokenize(text)\n",
    "    named_entities = getNER(sentences)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '?',\n",
       " 'Barrow-downs',\n",
       " 'Big',\n",
       " 'Bom',\n",
       " 'Bombadil',\n",
       " 'Bombadillo',\n",
       " 'Brandywine',\n",
       " 'Bucklebury',\n",
       " 'Comes',\n",
       " 'Forest',\n",
       " 'Frodo',\n",
       " 'Goldberry',\n",
       " 'Hey',\n",
       " 'Hill',\n",
       " 'Man',\n",
       " 'Merry',\n",
       " 'Old',\n",
       " 'People',\n",
       " 'Pippin',\n",
       " 'Reeds',\n",
       " 'River',\n",
       " 'Sam',\n",
       " 'Slender',\n",
       " 'Sun',\n",
       " 'Tom',\n",
       " 'Whoa',\n",
       " 'Willow',\n",
       " 'dol'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_ner = set(named_entities)\n",
    "split_ner_set = []\n",
    "for i in set_ner:\n",
    "  split = i.split(' ', 3)\n",
    "  split_ner_set.append(list(split))\n",
    "\n",
    "concat_list = [j for i in split_ner_set for j in i]\n",
    "split_ner_set = set(concat_list)\n",
    "split_ner_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "words_exceed= []\n",
    "noNERall = 0\n",
    "overall = 0\n",
    "wordsinfreqs = 0\n",
    "for sent in sentences:\n",
    "  sentence = nlp(sent)\n",
    "  for token in sentence:\n",
    "    t = token.text\n",
    "    l = token.lemma_\n",
    "    if (t.isalpha()):\n",
    "      if (len(t) <= 2):\n",
    "        continue\n",
    "      elif (t in split_ner_set):\n",
    "        noNERall += 1\n",
    "        continue\n",
    "      else:\n",
    "        if (l.lower() in freqs):\n",
    "          wordsinfreqs += 1\n",
    "          overall += 1\n",
    "        elif(l[0].isupper()):# decrease FN \n",
    "          noNERall += 1\n",
    "          continue\n",
    "        else:\n",
    "          overall += 1\n",
    "          words_exceed.append(l)\n",
    "noNERall += overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without NER\n",
      "Number of words in the text = 1558\n",
      "Number of words in freq list = 1387\n",
      "\n",
      "After NER\n",
      "Number of words in the text = 1458\n",
      "Number of words in freq list = 1387\n",
      "95% of words are in 10000 frequency list\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWithout NER\")\n",
    "print(\n",
    "        f\"Number of words in the text = {noNERall}\\nNumber of words in freq list = {wordsinfreqs}\")\n",
    "print(\"\\nAfter NER\")\n",
    "print(\n",
    "        f\"Number of words in the text = {overall}\\nNumber of words in freq list = {wordsinfreqs}\")\n",
    "print(f'{round(100*(wordsinfreqs/overall))}% of words are in {num_freq} frequency list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alder\n",
      "awakening\n",
      "battered\n",
      "beckoning\n",
      "breathlessly\n",
      "brightly\n",
      "bringing\n",
      "carelessly\n",
      "clamour\n",
      "crackle\n",
      "creak\n",
      "dancing\n",
      "derry\n",
      "dillo\n",
      "dong\n",
      "eastward\n",
      "eave\n",
      "fainter\n",
      "fal\n",
      "furtive\n",
      "glint\n",
      "gloome\n",
      "gnarl\n",
      "grassy\n",
      "grey\n",
      "halloo\n",
      "heartie\n",
      "hillside\n",
      "hoary\n",
      "hobbit\n",
      "honeycomb\n",
      "knobbly\n",
      "knoll\n",
      "lal\n",
      "leer\n",
      "loudly\n",
      "merrily\n",
      "muffled\n",
      "naught\n",
      "nonsensically\n",
      "pane\n",
      "relieved\n",
      "shadowy\n",
      "shaven\n",
      "silently\n",
      "singing\n",
      "slanting\n",
      "slop\n",
      "slump\n",
      "standstill\n",
      "starlight\n",
      "starry\n",
      "surprised\n",
      "swiftly\n",
      "tended\n",
      "tinder\n",
      "tremor\n",
      "violently\n",
      "weariness\n",
      "witless\n"
     ]
    }
   ],
   "source": [
    "words_exceed = set(words_exceed)\n",
    "words_exceed = sorted(words_exceed)\n",
    "for w in words_exceed:\n",
    "    print(w)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
